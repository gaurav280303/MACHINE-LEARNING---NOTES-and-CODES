{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwZN6PoNyaq8"
      },
      "source": [
        "BAG OF WORDS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-TlN8PtvICs",
        "outputId": "fb5eb26d-3a3c-4a11-a589-6d33cf4ec685"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary ['best' 'great' 'is' 'is great' 'is the' 'is the best' 'love' 'love pasta'\n",
            " 'love pizza' 'pasta' 'pasta is' 'pasta is great' 'pizza' 'pizza is'\n",
            " 'pizza is the' 'the' 'the best']\n",
            "\n",
            "BoW Matrix:\n",
            " [[0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0]\n",
            " [1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1]\n",
            " [0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0]\n",
            " [0 1 1 1 0 0 0 0 0 1 1 1 0 0 0 0 0]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "documents= [\n",
        "    \"I love pizza\",\n",
        "    \"Pizza is the best\",\n",
        "    \"I love pasta\",\n",
        "    \"Pasta is great\"\n",
        "]\n",
        "\n",
        "vectorizer = CountVectorizer(ngram_range=(1,3))\n",
        "\n",
        "x= vectorizer.fit_transform(documents)\n",
        "print(\"Vocabulary\", vectorizer.get_feature_names_out())\n",
        "print(\"\\nBoW Matrix:\\n\", x.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVZ1bTSJxp2I"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer:\n",
        "\n",
        "This line imports the CountVectorizer class from scikit-learn's feature_extraction.text module. CountVectorizer is a powerful tool used in natural language processing (NLP) to convert a collection of text documents into a matrix of token counts. This process is commonly known as creating a \"Bag of Words\" model.\n",
        "documents = [...]:\n",
        "\n",
        "This defines a Python list named documents. Each string in this list represents a short text document. These are the texts that we want to convert into a numerical representation.\n",
        "vectorizer = CountVectorizer(ngram_range=(1,3)):\n",
        "\n",
        "This line creates an instance of the CountVectorizer class. The ngram_range=(1,3) parameter is particularly important here:\n",
        "An \"n-gram\" is a contiguous sequence of n items from a given sample of text or speech.\n",
        "ngram_range=(1,3) tells the CountVectorizer to consider single words (unigrams, n=1), two-word phrases (bigrams, n=2), and three-word phrases (trigrams, n=3) as features. If this parameter were omitted, it would default to (1,1), meaning only single words would be counted.\n",
        "x = vectorizer.fit_transform(documents):\n",
        "\n",
        "This is a crucial step. The fit_transform method does two things:\n",
        "fit: It first fits the vectorizer to your documents. This means it analyzes all the text in documents to learn the entire vocabulary (all unique words and n-grams specified by ngram_range) present across all documents. Each unique word/n-gram found will become a feature (a column) in the resulting matrix.\n",
        "transform: After learning the vocabulary, it then transforms the documents into a numerical matrix. Each row in this matrix corresponds to a document, and each column corresponds to a word/n-gram from the learned vocabulary. The values in the matrix represent the count of how many times each word/n-gram appears in a given document.\n",
        "The result x is a sparse matrix, which is efficient for storing data where most values are zero (common in text data).\n",
        "print(\"Vocabulary\", vectorizer.get_feature_names_out()):\n",
        "\n",
        "vectorizer.get_feature_names_out() retrieves the list of all unique words and n-grams that the vectorizer learned during the fit step. These are the column names for your BoW matrix.\n",
        "print(\"\\nBoW Matrix:\\n\", x.toarray()):\n",
        "\n",
        "x.toarray() converts the sparse matrix x into a dense NumPy array, which is easier to inspect. This array is the final \"Bag of Words\" representation.\n",
        "Each row represents a document.\n",
        "Each column represents a word/n-gram from the vocabulary.\n",
        "The value at [row, column] indicates the count of that specific word/n-gram in that specific document.\n",
        "What is the \"Bag of Words\" (BoW) model?\n",
        "\n",
        "The Bag of Words model is a simplifying representation used in natural language processing and information retrieval. In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity. The goal is to represent text in a numerical format that machine learning algorithms can understand. Each document is vectorized by counting the occurrences of each word (or n-gram) from the vocabulary within it.\n",
        "\n",
        "What can I help you build?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkcMlP8Dzm2K"
      },
      "source": [
        "###TF -IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDGzx0D_zpgs"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Example usage, similar to CountVectorizer:\n",
        "# X_tfidf = tfidf_vectorizer.fit_transform(documents)\n",
        "# print(\"Vocabulary (TF-IDF):\", tfidf_vectorizer.get_feature_names_out())\n",
        "# print(\"\\nTF-IDF Matrix:\\n\", X_tfidf.toarray())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ihSr1Jz1iJr",
        "outputId": "d37a3aa3-bf38-4a48-a3c9-7b2f099a1cff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary ['best' 'great' 'is' 'is great' 'is the' 'is the best' 'love' 'love pasta'\n",
            " 'love pizza' 'pasta' 'pasta is' 'pasta is great' 'pizza' 'pizza is'\n",
            " 'pizza is the' 'the' 'the best']\n",
            "\n",
            "BoW Matrix:\n",
            " [[0.         0.         0.         0.70710678 0.         0.70710678\n",
            "  0.        ]\n",
            " [0.55528266 0.         0.43779123 0.         0.         0.43779123\n",
            "  0.55528266]\n",
            " [0.         0.         0.         0.70710678 0.70710678 0.\n",
            "  0.        ]\n",
            " [0.         0.66767854 0.52640543 0.         0.52640543 0.\n",
            "  0.        ]]\n"
          ]
        }
      ],
      "source": [
        "documents= [\n",
        "    \"I love pizza\",\n",
        "    \"Pizza is the best\",\n",
        "    \"I love pasta\",\n",
        "    \"Pasta is great\"\n",
        "]\n",
        "\n",
        "\n",
        "x= tfidf_vectorizer.fit_transform(documents)\n",
        "print(\"Vocabulary\", vectorizer.get_feature_names_out())\n",
        "print(\"\\nBoW Matrix:\\n\", x.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kst4qFV4uNg"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PQj-EKR0wjb"
      },
      "source": [
        "###KRISH NAIK NLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gF9A_BTq0hP6"
      },
      "outputs": [],
      "source": [
        "corpus=\"Hello Welcome to Gaurav. Suppert his laptop!. He lost it*\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7o1Qx0X70wA4",
        "outputId": "6691d16c-d1cf-4edc-e885-c88a95e01348"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello Welcome to Gaurav. Suppert his laptop!. He lost it*'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MF7ZnrsL05pS"
      },
      "source": [
        "TOKENIZATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8tc1QsX05HG"
      },
      "outputs": [],
      "source": [
        "#Para to sentence\n",
        "from nltk.tokenize import sent_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZB2VMCHC1H4P",
        "outputId": "27ef3a47-365d-41ca-89fc-a03c20ed4d5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "documents=sent_tokenize(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAgwOG0x1w7n",
        "outputId": "5771c0c6-ccec-46ab-f77b-d6c8dfb678d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello Welcome to Gaurav.', 'Suppert his laptop!.', 'He lost it*']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "print(documents)\n",
        "type(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8Sb2jXD2JLw",
        "outputId": "ba756262-3036-4cf6-e9f8-39112e9d1301"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello Welcome to Gaurav.\n",
            "Suppert his laptop!.\n",
            "He lost it*\n"
          ]
        }
      ],
      "source": [
        "for sentence in documents:\n",
        "  print(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2k4OoX0W2P4f"
      },
      "outputs": [],
      "source": [
        "#Para to word\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMMLFo742eDw",
        "outputId": "b08a8b9d-ee13-4e8a-9db9-f049704ef57d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'Welcome',\n",
              " 'to',\n",
              " 'Gaurav',\n",
              " '.',\n",
              " 'Suppert',\n",
              " 'his',\n",
              " 'laptop',\n",
              " '!',\n",
              " '.',\n",
              " 'He',\n",
              " 'lost',\n",
              " 'it',\n",
              " '*']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "word_tokenize(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bX4VZiZ2tVL",
        "outputId": "8ec3b664-3bef-4a16-e065-95a967b10192"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'Welcome', 'to', 'Gaurav', '.']\n",
            "['Suppert', 'his', 'laptop', '!', '.']\n",
            "['He', 'lost', 'it', '*']\n"
          ]
        }
      ],
      "source": [
        "for word in documents:\n",
        "  print(word_tokenize(word))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-fg1JMz25R2"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Ro_z5c93Uzv",
        "outputId": "1fab1c12-006e-4bda-942c-c3c19bd12263"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'Welcome',\n",
              " 'to',\n",
              " 'Gaurav.',\n",
              " 'Suppert',\n",
              " 'his',\n",
              " 'laptop',\n",
              " '!',\n",
              " '.',\n",
              " 'He',\n",
              " 'lost',\n",
              " 'it*']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "tokenizer = TreebankWordTokenizer()\n",
        "tokenizer.tokenize(corpus)\n",
        "#It does not treat fullstop as separate token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPnjsNUs4DRC"
      },
      "source": [
        "##STEMMING\n",
        "\n",
        "eating, eaten, eat = eat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXgfyX7l44XU"
      },
      "outputs": [],
      "source": [
        "words=['eating', 'eaten', 'eat', 'eats', 'writing', 'write', 'writes', 'go', 'gone', 'going']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdB_tTFu6PuU"
      },
      "source": [
        "####TECHNIQUE 1  PORTER STEMMER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEokhTWC5SCb"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_0aJ54d5ao2"
      },
      "outputs": [],
      "source": [
        "portstem=PorterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6a6p-PE5gg6",
        "outputId": "25f43822-3d34-4d75-8930-f0a390cb2091"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating---->eat\n",
            "eaten---->eaten\n",
            "eat---->eat\n",
            "eats---->eat\n",
            "writing---->write\n",
            "write---->write\n",
            "writes---->write\n",
            "go---->go\n",
            "gone---->gone\n",
            "going---->go\n"
          ]
        }
      ],
      "source": [
        "for word in words:\n",
        "  print(word+\"---->\"+portstem.stem(word))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1w7jF37n6EoT",
        "outputId": "02875a21-e97b-41ee-90a6-6e7bba79abdd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'congratul'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "portstem.stem('congratulations')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaF3g4SA6ZlE"
      },
      "source": [
        "#####TECHNIQUE 2 REGEXPSTEMMER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNzRxDu-6Lx2"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import RegexpStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqivY3tq634H"
      },
      "outputs": [],
      "source": [
        "regstem=RegexpStemmer('ing$|s$|e$|able$', min=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8aWSWFer7D9s",
        "outputId": "c97aec75-4d13-4669-906e-a1e621e6de9b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'eat'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "regstem.stem('eating')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xlcERv887mBe",
        "outputId": "d8aaca66-0efc-4348-ba9e-863612a8e35f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ingeat'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "regstem.stem('ingeating') #because we  put $ at last in above cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4bsEUDl7Q0S",
        "outputId": "9f04708c-42c2-49d7-9df7-3d0b5c781af0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating---->eat\n",
            "eaten---->eaten\n",
            "eat---->eat\n",
            "eats---->eat\n",
            "writing---->writ\n",
            "write---->writ\n",
            "writes---->write\n",
            "go---->go\n",
            "gone---->gon\n",
            "going---->go\n"
          ]
        }
      ],
      "source": [
        "for word in words:\n",
        "  print(word + \"---->\" + regstem.stem(word))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYoqAW8i757t"
      },
      "source": [
        "#### TECHNIQUE 3 SNOWBALL STEMMER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mD7cGvJr7-eu"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import SnowballStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXbAmgai8Nq9"
      },
      "outputs": [],
      "source": [
        "snowstem=SnowballStemmer('english') #predefined language"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xriOyCHj8YL8",
        "outputId": "140848ef-a6d7-4fab-f680-71e2abb48e54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating---->eat\n",
            "eaten---->eaten\n",
            "eat---->eat\n",
            "eats---->eat\n",
            "writing---->write\n",
            "write---->write\n",
            "writes---->write\n",
            "go---->go\n",
            "gone---->gone\n",
            "going---->go\n"
          ]
        }
      ],
      "source": [
        "for word in words:\n",
        "  print(word+\"---->\" + snowstem.stem(word))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZHAwyShl8rBI",
        "outputId": "aa8784f0-e969-49cd-d081-006819aeb5d5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'histori'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "snowstem.stem('history')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSlTcpmDAx_y"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpcA0rmi9P0u"
      },
      "source": [
        "##LEMMATIZATION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6HTxNx-9d6e"
      },
      "source": [
        "####Technique 1 WORDNET LEMMATIZER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ei-wd_dT9Vw1"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GpRokC1-Mqn"
      },
      "outputs": [],
      "source": [
        "lemmatizer=WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_OZlj6s-Z-p",
        "outputId": "1e3631da-80ee-4363-f534-4fbde83dd78f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DG76g4No_HYl",
        "outputId": "ae012f53-8029-47ed-d1ad-61dc68e06767"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'going'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "lemmatizer.lemmatize('going', pos='a') #a=adjective"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "FhxY57od_LnM",
        "outputId": "5d8522fe-1a90-4218-9515-c66be2a6d71f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'go'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "lemmatizer.lemmatize('going', pos='v') #v= verb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZ-KgMEn_TSs",
        "outputId": "1b36ced2-1ae5-41f0-d69a-272adc697070"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating---->eating\n",
            "eaten---->eaten\n",
            "eat---->eat\n",
            "eats---->eats\n",
            "writing---->writing\n",
            "write---->write\n",
            "writes---->writes\n",
            "go---->go\n",
            "gone---->gone\n",
            "going---->going\n"
          ]
        }
      ],
      "source": [
        "for word in words:\n",
        "  print(word+\"---->\" + lemmatizer.lemmatize(word, pos='n')) #n= noun"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUoCc7FM_0zs",
        "outputId": "c7b6c618-838a-4dd0-98b2-ef318e100dcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating---->eat\n",
            "eaten---->eat\n",
            "eat---->eat\n",
            "eats---->eat\n",
            "writing---->write\n",
            "write---->write\n",
            "writes---->write\n",
            "go---->go\n",
            "gone---->go\n",
            "going---->go\n"
          ]
        }
      ],
      "source": [
        "for word in words:\n",
        "  print(word+\"---->\" + lemmatizer.lemmatize(word, pos='v')) #v= verb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnOkjSXLAIDC",
        "outputId": "75fbbbf2-49a3-4c69-b83a-e92ccda3a520"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('failrly', 'sportingly')"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "lemmatizer.lemmatize(\"failrly\"), lemmatizer.lemmatize(\"sportingly\", pos='v')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####DIFFERENCE BETWEEN STEMMING AND LEMMATIZATION\n",
        "\n",
        "ey seem similar here, but wait...\n",
        "\n",
        "Part 2: The Key Difference - Smart vs Dumb ‚úÇÔ∏èüìö\n",
        "Stemming = The Fast, Dumb Chopper\n",
        "How it works: Simple rules like \"remove 'ing', remove 's', remove 'ed'\"\n",
        "\n",
        "Fast but: Sometimes wrong!\n",
        "\n",
        "Stemming Examples:\n",
        "\n",
        "text\n",
        "\"running\" ‚Üí \"run\" ‚úÖ (Good!)\n",
        "\"running\" ‚Üí \"run\" ‚úÖ (Good!)\n",
        "\"runner\" ‚Üí \"run\" ‚ö†Ô∏è (A runner is a person, not an action!)\n",
        "\"ran\" ‚Üí \"ran\" ‚ùå (Rule doesn't catch irregular verbs)\n",
        "\"better\" ‚Üí \"better\" ‚ùå (No rule for comparative)\n",
        "Stemming says: \"I see 'ing' at end ‚Üí chop it off!\"\n",
        "\n",
        "Lemmatization = The Smart Dictionary Lookup\n",
        "How it works: Uses vocabulary and word analysis\n",
        "\n",
        "Slow but: Usually correct!\n",
        "\n",
        "Needs to know: Is this word a noun? verb? adjective?\n",
        "\n",
        "Lemmatization Examples:\n",
        "\n",
        "text\n",
        "\"running\" (verb) ‚Üí \"run\" ‚úÖ\n",
        "\"running\" (noun: \"daily running\") ‚Üí \"running\" ‚úÖ\n",
        "\"runner\" ‚Üí \"runner\" ‚úÖ (Kept as noun!)\n",
        "\"ran\" ‚Üí \"run\" ‚úÖ (Knows irregular verbs!)\n",
        "\"better\" (adj) ‚Üí \"good\" ‚úÖ (Knows \"good, better, best\")\n",
        "\"better\" (verb: \"to better yourself\") ‚Üí \"better\" ‚úÖ\n",
        "Lemmatization says: \"Let me check the dictionary and see what part of speech this is!\"\n",
        "\n",
        "Part 3: Real Examples - Spot the Difference!\n",
        "Example 1: \"Caring\"\n",
        "text\n",
        "Sentence: \"She is caring for her plants\"\n",
        "\n",
        "Stemming: \"caring\" ‚Üí \"car\" ‚ùå (Oops! Wrong meaning!)\n",
        "Lemmatization: \"caring\" (verb) ‚Üí \"care\" ‚úÖ\n",
        "Why? Stemmer sees \"ing\" ‚Üí chops ‚Üí \"car\" (like automobile!)\n",
        "Lemmatizer checks: \"caring\" is verb form of \"care\" ‚Üí \"care\""
      ],
      "metadata": {
        "id": "Sy4BocphYqLo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwMMhYRAAxPV"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNQg616vAr3U"
      },
      "source": [
        "##STOPWORDS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UELJTChcAucj"
      },
      "outputs": [],
      "source": [
        "#Speech\n",
        "speech=\"\"\"Good morning, family, friends, faculty, and fellow graduates.\n",
        "\n",
        "\n",
        "Well, we did it. We all accomplished one of the major early milestones of our lives: high school graduation. This is a major step in the journey of our lives, one that should be recognized for its immense significance. It is an act not only of personal commitment, but also one of pride. We all worked hard to get to this day, and our work did not go to waste. A high school diploma is a wonderful tool in this world, one that opens many doors of opportunity for anyone who is lucky enough to have one.\n",
        "\n",
        "\n",
        "But graduation is not an end goal in itself; it is instead a part of the larger journey of life. Wherever your future takes you, let it take you somewhere. Life is a journey, and all accomplishments we achieve during its course should be taken as starting points for further achievements. Our graduation should serve as such a launching point, projecting us to wherever our futures are meant to take us, whether we land ourselves a career, take up a trade, or continue our education at college or Vocational/technical School.\n",
        "\n",
        "\n",
        "But before we can begin to reach for the stars, there is one more personal milestone that we all need to reach. Most people who graduate from high school experience only one graduation ‚Äî that from high school. But we all have one more shortly ahead of us.\n",
        "\n",
        "\n",
        "We‚Äôve already shown our commitment to personal growth through making it to this ceremony today, but soon, all of us will experience another ceremony when we graduate from our programs. As I said before, life is a journey ‚Äî we don‚Äôt stop growing once we get our diplomas. Life is about growing, and being in our programs gives each of us new opportunities to continue growing and to learn new skills that we will carry with us for the rest of our lives.\n",
        "\n",
        "\n",
        "And we don‚Äôt have to stop there! This graduation has already shown us how capable we all are of accomplishing our goals when we commit ourselves to them. I hope all of us here today can take this personal accomplishment as an example of how anything is truly possible when we put our minds to it. As we all continue on in our lives, let us take each new problem on with confidence, knowing that we have achieved great heights and are equipped with the necessary tools to tackle our futures.\n",
        "\n",
        "\n",
        "The road that lies ahead won‚Äôt be easy. There will be obstacles and missed exits, potholes and roadblocks. There will be times when each of us will feel like we cannot possibly go on. There will be times when each of us will think he or she is alone, backed against the wall. But we are not alone ‚Äî we are all on this journey together. We‚Äôve made it this far ‚Äî why should we back out now? Nothing worthwhile is easy, and that includes making the most out of our futures. But that doesn‚Äôt mean we give up on ourselves. We will keep pushing because we know we can achieve our dreams, and because we are worth it.\n",
        "\n",
        "From this day forward, let us make each decision with our best interests in mind. Let us  believe in ourselves so that we may reach our goals and fulfill our dreams. Let us be the best that we can be so that we may fill our lives and the lives of those closest to us with happiness and with pride. We‚Äôve already taken the first step by making it to this ceremony today ‚Äî now, it‚Äôs time to take the next steps in the journey that is our lives and begin to build our futures.\n",
        "\n",
        "Congratulations to the class of 2013!\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "hBKrvl7RNkO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "ybJDUXkNNqDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgKt_vJINtpW",
        "outputId": "deafd01f-64b0-44cd-daa3-b72fa4064a94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords.words('english')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNaC4JUWNzIo",
        "outputId": "d930d73e-8076-450c-8dd2-03bfc1bf7fd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ain',\n",
              " 'all',\n",
              " 'am',\n",
              " 'an',\n",
              " 'and',\n",
              " 'any',\n",
              " 'are',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'as',\n",
              " 'at',\n",
              " 'be',\n",
              " 'because',\n",
              " 'been',\n",
              " 'before',\n",
              " 'being',\n",
              " 'below',\n",
              " 'between',\n",
              " 'both',\n",
              " 'but',\n",
              " 'by',\n",
              " 'can',\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'd',\n",
              " 'did',\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'do',\n",
              " 'does',\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'doing',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'down',\n",
              " 'during',\n",
              " 'each',\n",
              " 'few',\n",
              " 'for',\n",
              " 'from',\n",
              " 'further',\n",
              " 'had',\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'has',\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'have',\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'having',\n",
              " 'he',\n",
              " \"he'd\",\n",
              " \"he'll\",\n",
              " 'her',\n",
              " 'here',\n",
              " 'hers',\n",
              " 'herself',\n",
              " \"he's\",\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'i',\n",
              " \"i'd\",\n",
              " 'if',\n",
              " \"i'll\",\n",
              " \"i'm\",\n",
              " 'in',\n",
              " 'into',\n",
              " 'is',\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'it',\n",
              " \"it'd\",\n",
              " \"it'll\",\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " \"i've\",\n",
              " 'just',\n",
              " 'll',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'me',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'more',\n",
              " 'most',\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'my',\n",
              " 'myself',\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'now',\n",
              " 'o',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'once',\n",
              " 'only',\n",
              " 'or',\n",
              " 'other',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 're',\n",
              " 's',\n",
              " 'same',\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'she',\n",
              " \"she'd\",\n",
              " \"she'll\",\n",
              " \"she's\",\n",
              " 'should',\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " \"should've\",\n",
              " 'so',\n",
              " 'some',\n",
              " 'such',\n",
              " 't',\n",
              " 'than',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " \"they'd\",\n",
              " \"they'll\",\n",
              " \"they're\",\n",
              " \"they've\",\n",
              " 'this',\n",
              " 'those',\n",
              " 'through',\n",
              " 'to',\n",
              " 'too',\n",
              " 'under',\n",
              " 'until',\n",
              " 'up',\n",
              " 've',\n",
              " 'very',\n",
              " 'was',\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'we',\n",
              " \"we'd\",\n",
              " \"we'll\",\n",
              " \"we're\",\n",
              " 'were',\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " \"we've\",\n",
              " 'what',\n",
              " 'when',\n",
              " 'where',\n",
              " 'which',\n",
              " 'while',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " 'y',\n",
              " 'you',\n",
              " \"you'd\",\n",
              " \"you'll\",\n",
              " 'your',\n",
              " \"you're\",\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " \"you've\"]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer=PorterStemmer()"
      ],
      "metadata": {
        "id": "YVF6Z4j6N99i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences=nltk.sent_tokenize(speech)  #We divided the paragraph in to sentencess"
      ],
      "metadata": {
        "id": "XnVQAkV6ODi1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying Porter Stemmer"
      ],
      "metadata": {
        "id": "Ab-BAek3RZCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Apply stopwords and filter and then apply stemming\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "    words = nltk.word_tokenize(sentences[i])\n",
        "    words = [stemmer.stem(word) for word in words if word.lower() not in set(stopwords.words('english'))]\n",
        "    sentences[i] = ' '.join(words)"
      ],
      "metadata": {
        "id": "fprR-CWxOUX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fprR-CWxOU_"
      },
      "source": [
        "##Apply stopwords and filter and then apply stemming\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "  words=nltk.word_tokenize(sentences[i])  #Converting sententes into list of  words\n",
        "  words=[stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))] #List comprehension , only stemming those words which are not present in stopwords\n",
        "  sentences[i]=' '.join(words) #join words, converting all the list of words in to sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences #All the words which are not in stopwords , have been stemmed\n",
        "#Porter stemmer doed not look , we will try Snowball stemmer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtpFe2y1P79K",
        "outputId": "1757cffb-83eb-47d6-8148-18c4930d8fed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['good morn , famili , friend , faculti , fellow graduat .',\n",
              " 'well , .',\n",
              " 'we accomplish one major earli mileston live : high school graduat .',\n",
              " 'thi major step journey live , one recogn immens signific .',\n",
              " 'it act person commit , also one pride .',\n",
              " 'we work hard get day , work go wast .',\n",
              " 'a high school diploma wonder tool world , one open mani door opportun anyon lucki enough one .',\n",
              " 'but graduat end goal ; instead part larger journey life .',\n",
              " 'wherev futur take , let take somewher .',\n",
              " 'life journey , accomplish achiev cours taken start point achiev .',\n",
              " 'our graduat serv launch point , project us wherev futur meant take us , whether land career , take trade , continu educ colleg vocational/techn school .',\n",
              " 'but begin reach star , one person mileston need reach .',\n",
              " 'most peopl graduat high school experi one graduat ‚Äî high school .',\n",
              " 'but one shortli ahead us .',\n",
              " 'we ‚Äô alreadi shown commit person growth make ceremoni today , soon , us experi anoth ceremoni graduat program .',\n",
              " 'as i said , life journey ‚Äî ‚Äô stop grow get diploma .',\n",
              " 'life grow , program give us new opportun continu grow learn new skill carri us rest live .',\n",
              " 'and ‚Äô stop !',\n",
              " 'thi graduat alreadi shown us capabl accomplish goal commit .',\n",
              " 'i hope us today take person accomplish exampl anyth truli possibl put mind .',\n",
              " 'as continu live , let us take new problem confid , know achiev great height equip necessari tool tackl futur .',\n",
              " 'the road lie ahead ‚Äô easi .',\n",
              " 'there obstacl miss exit , pothol roadblock .',\n",
              " 'there time us feel like possibl go .',\n",
              " 'there time us think alon , back wall .',\n",
              " 'but alon ‚Äî journey togeth .',\n",
              " 'we ‚Äô made far ‚Äî back ?',\n",
              " 'noth worthwhil easi , includ make futur .',\n",
              " 'but ‚Äô mean give .',\n",
              " 'we keep push know achiev dream , worth .',\n",
              " 'from day forward , let us make decis best interest mind .',\n",
              " 'let us believ may reach goal fulfil dream .',\n",
              " 'let us best may fill live live closest us happi pride .',\n",
              " 'we ‚Äô alreadi taken first step make ceremoni today ‚Äî , ‚Äô time take next step journey live begin build futur .',\n",
              " 'congratul class 2013 !']"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "snowstem=SnowballStemmer('english')"
      ],
      "metadata": {
        "id": "b0MTzF9hQI5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying Snowball Stemming"
      ],
      "metadata": {
        "id": "wO9J3IZzRT3-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(sentences)):\n",
        "  words=word_tokenize(sentences[i])\n",
        "  words=[snowstem.stem(word) for word in words if word not in set(stopwords.words('english'))] #List comprehension , only stemming those words which are not present in stopwords\n",
        "  sentences[i] = ' '.join(words) #join words, converting all the list of words in to sentences)\n"
      ],
      "metadata": {
        "id": "ORHXMfF0Qen7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeA1o1VvRJSH",
        "outputId": "974c4e48-8cd9-4eea-ea3c-3ca3b05bc6e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['good morn , famili , friend , faculti , fellow graduat .',\n",
              " 'well , .',\n",
              " 'accomplish one major ear mileston live : high school graduat .',\n",
              " 'thi major step journey live , one recogn immen signif .',\n",
              " 'act person commit , also one pride .',\n",
              " 'work hard get day , work go wast .',\n",
              " 'high school diploma wonder tool world , one open mani door opportun anyon lucki enough one .',\n",
              " 'graduat end goal ; instead part larger journey life .',\n",
              " 'wherev futur take , let take somewh .',\n",
              " 'life journey , accomplish achiev cour taken start point achiev .',\n",
              " 'graduat serv launch point , project us wherev futur meant take us , whether land career , take trade , continu educ colleg vocational/techn school .',\n",
              " 'begin reach star , one person mileston need reach .',\n",
              " 'peopl graduat high school experi one graduat ‚Äî high school .',\n",
              " 'one short ahead us .',\n",
              " '‚Äô alreadi shown commit person growth make ceremoni today , soon , us experi anoth ceremoni graduat program .',\n",
              " 'said , life journey ‚Äî ‚Äô stop grow get diploma .',\n",
              " 'life grow , program give us new opportun continu grow learn new skill carri us rest live .',\n",
              " '‚Äô stop !',\n",
              " 'thi graduat alreadi shown us capabl accomplish goal commit .',\n",
              " 'hope us today take person accomplish exampl anyth truli possibl put mind .',\n",
              " 'continu live , let us take new problem confid , know achiev great height equip necessari tool tackl futur .',\n",
              " 'road lie ahead ‚Äô easi .',\n",
              " 'obstacl miss exit , pothol roadblock .',\n",
              " 'time us feel like possibl go .',\n",
              " 'time us think alon , back wall .',\n",
              " 'alon ‚Äî journey togeth .',\n",
              " '‚Äô made far ‚Äî back ?',\n",
              " 'noth worthwhil easi , includ make futur .',\n",
              " '‚Äô mean give .',\n",
              " 'keep push know achiev dream , worth .',\n",
              " 'day forward , let us make deci best interest mind .',\n",
              " 'let us believ may reach goal fulfil dream .',\n",
              " 'let us best may fill live live closest us happi pride .',\n",
              " '‚Äô alreadi taken first step make ceremoni today ‚Äî , ‚Äô time take next step journey live begin build futur .',\n",
              " 'congratul class 2013 !']"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying Lammetization"
      ],
      "metadata": {
        "id": "DznOgqbEReHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemma=WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "nYakBd4ARgxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(sentences)):\n",
        "  words=word_tokenize(sentences[i])\n",
        "  words=[lemma.lemmatize(word.lower(), pos='v') for word in words if word not in set(stopwords.words('english'))] #List comprehension , only lemmatizing those words which are not present in stopwords\n",
        "  sentences[i] = ' '.join(words) #join words, converting all the list of words in to sentences)\n"
      ],
      "metadata": {
        "id": "4_DH4KDmRuP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "for i in range(len(sentences)):\n",
        "\n",
        "This initiates a for loop that iterates through the sentences list using an index i. For each iteration, i will represent the current index of the sentence being processed.\n",
        "words=word_tokenize(sentences[i])\n",
        "\n",
        "Inside the loop, for each sentences[i] (which is a single sentence string), word_tokenize() is called. This function from the NLTK library breaks down the sentence into a list of individual words (tokens). For example, \"Hello World!\" would become ['Hello', 'World', '!'].\n",
        "The resulting list of words is stored in the words variable.\n",
        "words=[lemma.lemmatize(word.lower(), pos='v') for word in words if word not in set(stopwords.words('english'))]\n",
        "\n",
        "This is a list comprehension that performs two main NLP operations: stopword removal and lemmatization.\n",
        "for word in words: It iterates through each word in the words list obtained from the previous step.\n",
        "if word not in set(stopwords.words('english')): This is the stopword removal part. Before processing a word, it checks if the lowercase version of the word is present in a set of common English stopwords (like \"a\", \"the\", \"is\", etc.). If the word is a stopword, it is filtered out and not included in the new list.\n",
        "word.lower(): Converts the current word to lowercase. This ensures that words like \"The\" and \"the\" are treated as the same, which is crucial for consistent processing and matching against stopwords or for lemmatization.\n",
        "lemma.lemmatize(word.lower(), pos='v'): This is the lemmatization step. The lemmatizer object (an instance of WordNetLemmatizer) takes the lowercase word and converts it to its base or dictionary form (its lemma). The pos='v' argument is very important here; it tells the lemmatizer to treat the word as a verb, which influences how it finds the base form (e.g., \"eating\", \"ate\", \"eaten\" all become \"eat\" when pos='v' is used). Without pos='v', the lemmatizer often defaults to noun (pos='n') which might not give the desired result for verbs.\n",
        "The output of this list comprehension is a new list of words, where stopwords have been removed and the remaining words have been lemmatized to their verb base form.\n",
        "sentences[i] = ' '.join(words)\n",
        "\n",
        "After processing, the list of lemmatized and stopword-removed words (words) is joined back into a single string. The ' ' specifies that a space should be used as the separator between the words.\n",
        "This new, cleaned string (sentence) then replaces the original sentence at the i-th position in the sentences list"
      ],
      "metadata": {
        "id": "apIxOhcTXpqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1odGQh6SBYC",
        "outputId": "b709b296-56fa-4e13-e57c-2cbe125cf28c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['good morn , famili , friend , faculti , fellow graduat .',\n",
              " 'well , .',\n",
              " 'accomplish one major ear mileston live : high school graduat .',\n",
              " 'thi major step journey live , one recogn immen signif .',\n",
              " 'act person commit , also one pride .',\n",
              " 'work hard get day , work go wast .',\n",
              " 'high school diploma wonder tool world , one open mani door opportun anyon lucki enough one .',\n",
              " 'graduat end goal ; instead part larger journey life .',\n",
              " 'wherev futur take , let take somewh .',\n",
              " 'life journey , accomplish achiev cour take start point achiev .',\n",
              " 'graduat serv launch point , project us wherev futur mean take us , whether land career , take trade , continu educ colleg vocational/techn school .',\n",
              " 'begin reach star , one person mileston need reach .',\n",
              " 'peopl graduat high school experi one graduat ‚Äî high school .',\n",
              " 'one short ahead us .',\n",
              " '‚Äô alreadi show commit person growth make ceremoni today , soon , us experi anoth ceremoni graduat program .',\n",
              " 'say , life journey ‚Äî ‚Äô stop grow get diploma .',\n",
              " 'life grow , program give us new opportun continu grow learn new skill carri us rest live .',\n",
              " '‚Äô stop !',\n",
              " 'thi graduat alreadi show us capabl accomplish goal commit .',\n",
              " 'hope us today take person accomplish exampl anyth truli possibl put mind .',\n",
              " 'continu live , let us take new problem confid , know achiev great height equip necessari tool tackl futur .',\n",
              " 'road lie ahead ‚Äô easi .',\n",
              " 'obstacl miss exit , pothol roadblock .',\n",
              " 'time us feel like possibl go .',\n",
              " 'time us think alon , back wall .',\n",
              " 'alon ‚Äî journey togeth .',\n",
              " '‚Äô make far ‚Äî back ?',\n",
              " 'noth worthwhil easi , includ make futur .',\n",
              " '‚Äô mean give .',\n",
              " 'keep push know achiev dream , worth .',\n",
              " 'day forward , let us make deci best interest mind .',\n",
              " 'let us believ may reach goal fulfil dream .',\n",
              " 'let us best may fill live live closest us happi pride .',\n",
              " '‚Äô alreadi take first step make ceremoni today ‚Äî , ‚Äô time take next step journey live begin build futur .',\n",
              " 'congratul class 2013 !']"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "POS_TAG --(PARTS OF SPEECH)TAGS"
      ],
      "metadata": {
        "id": "RtfffmR1So9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We will find pos tags"
      ],
      "metadata": {
        "id": "vtYqteEFUfXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEoe6tarVN2e",
        "outputId": "b5ad65aa-46b0-4cb1-de3b-58bec87fa513"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(sentences)):\n",
        "  words=word_tokenize(sentences[i])\n",
        "  words=[word.lower() for word in words if word not in set(stopwords.words('english'))] #List comprehension , only lemmatizing those words which are not present in stopwords\n",
        "  #sentences[i] = ' '.join(words) #join words, converting all the list of words in to sentences)\n",
        "  pos_tag_word=nltk.pos_tag(words)  #Eah word show which POS it is\n",
        "  print(f\"POS Tags for sentence {i+1}: {pos_tag_word}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oszj50iaUKsq",
        "outputId": "2ab5797a-7af5-41cc-d36f-582f13b809fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tags for sentence 1: [('good', 'JJ'), ('morn', 'NN'), (',', ','), ('famili', 'NN'), (',', ','), ('friend', 'NN'), (',', ','), ('faculti', 'NN'), (',', ','), ('fellow', 'JJ'), ('graduat', 'NN'), ('.', '.')]\n",
            "POS Tags for sentence 2: [('well', 'RB'), (',', ','), ('.', '.')]\n",
            "POS Tags for sentence 3: [('accomplish', 'VB'), ('one', 'CD'), ('major', 'JJ'), ('ear', 'JJ'), ('mileston', 'NN'), ('live', 'NN'), (':', ':'), ('high', 'JJ'), ('school', 'NN'), ('graduat', 'NN'), ('.', '.')]\n",
            "POS Tags for sentence 4: [('thi', 'NN'), ('major', 'JJ'), ('step', 'NN'), ('journey', 'NN'), ('live', 'VBP'), (',', ','), ('one', 'CD'), ('recogn', 'NN'), ('immen', 'NNS'), ('signif', 'VBP'), ('.', '.')]\n",
            "POS Tags for sentence 5: [('act', 'NN'), ('person', 'NN'), ('commit', 'NN'), (',', ','), ('also', 'RB'), ('one', 'CD'), ('pride', 'NN'), ('.', '.')]\n",
            "POS Tags for sentence 6: [('work', 'NN'), ('hard', 'JJ'), ('get', 'NN'), ('day', 'NN'), (',', ','), ('work', 'NN'), ('go', 'VB'), ('wast', 'NN'), ('.', '.')]\n",
            "POS Tags for sentence 7: [('high', 'JJ'), ('school', 'NN'), ('diploma', 'NN'), ('wonder', 'NN'), ('tool', 'NN'), ('world', 'NN'), (',', ','), ('one', 'CD'), ('open', 'JJ'), ('mani', 'NN'), ('door', 'NN'), ('opportun', 'IN'), ('anyon', 'JJ'), ('lucki', 'CC'), ('enough', 'JJ'), ('one', 'CD'), ('.', '.')]\n",
            "POS Tags for sentence 8: [('graduat', 'JJ'), ('end', 'NN'), ('goal', 'NN'), (';', ':'), ('instead', 'RB'), ('part', 'NN'), ('larger', 'JJR'), ('journey', 'NN'), ('life', 'NN'), ('.', '.')]\n",
            "POS Tags for sentence 9: [('wherev', 'JJ'), ('futur', 'NNS'), ('take', 'VBP'), (',', ','), ('let', 'VB'), ('take', 'VB'), ('somewh', 'NN'), ('.', '.')]\n",
            "POS Tags for sentence 10: [('life', 'NN'), ('journey', 'NN'), (',', ','), ('accomplish', 'JJ'), ('achiev', 'IN'), ('cour', 'NNS'), ('take', 'VBP'), ('start', 'JJ'), ('point', 'NN'), ('achiev', 'NN'), ('.', '.')]\n",
            "POS Tags for sentence 11: [('graduat', 'NN'), ('serv', 'NN'), ('launch', 'NN'), ('point', 'NN'), (',', ','), ('project', 'VBP'), ('us', 'PRP'), ('wherev', 'JJ'), ('futur', 'JJ'), ('mean', 'NNS'), ('take', 'VBP'), ('us', 'PRP'), (',', ','), ('whether', 'IN'), ('land', 'NN'), ('career', 'NN'), (',', ','), ('take', 'VB'), ('trade', 'NN'), (',', ','), ('continu', 'VBP'), ('educ', 'JJ'), ('colleg', 'NN'), ('vocational/techn', 'JJ'), ('school', 'NN'), ('.', '.')]\n",
            "POS Tags for sentence 12: [('begin', 'VB'), ('reach', 'NN'), ('star', 'NN'), (',', ','), ('one', 'CD'), ('person', 'NN'), ('mileston', 'FW'), ('need', 'NN'), ('reach', 'NN'), ('.', '.')]\n",
            "POS Tags for sentence 13: [('peopl', 'NN'), ('graduat', 'NN'), ('high', 'JJ'), ('school', 'NN'), ('experi', 'VBP'), ('one', 'CD'), ('graduat', 'NN'), ('‚Äî', 'NNP'), ('high', 'JJ'), ('school', 'NN'), ('.', '.')]\n",
            "POS Tags for sentence 14: [('one', 'CD'), ('short', 'JJ'), ('ahead', 'RB'), ('us', 'PRP'), ('.', '.')]\n",
            "POS Tags for sentence 15: [('‚Äô', 'JJ'), ('alreadi', 'NN'), ('show', 'NN'), ('commit', 'VBP'), ('person', 'NN'), ('growth', 'NN'), ('make', 'VBP'), ('ceremoni', 'NN'), ('today', 'NN'), (',', ','), ('soon', 'RB'), (',', ','), ('us', 'PRP'), ('experi', 'VBP'), ('anoth', 'DT'), ('ceremoni', 'JJ'), ('graduat', 'NN'), ('program', 'NN'), ('.', '.')]\n",
            "POS Tags for sentence 16: [('say', 'NN'), (',', ','), ('life', 'NN'), ('journey', 'NN'), ('‚Äî', 'NNP'), ('‚Äô', 'NNP'), ('stop', 'VB'), ('grow', 'NN'), ('get', 'NN'), ('diploma', 'NN'), ('.', '.')]\n",
            "POS Tags for sentence 17: [('life', 'NN'), ('grow', 'NN'), (',', ','), ('program', 'NN'), ('give', 'VBP'), ('us', 'PRP'), ('new', 'JJ'), ('opportun', 'JJ'), ('continu', 'NN'), ('grow', 'NN'), ('learn', 'VBP'), ('new', 'JJ'), ('skill', 'NN'), ('carri', 'VBP'), ('us', 'PRP'), ('rest', 'RBS'), ('live', 'JJ'), ('.', '.')]\n",
            "POS Tags for sentence 18: [('‚Äô', 'JJ'), ('stop', 'NN'), ('!', '.')]\n",
            "POS Tags for sentence 19: [('thi', 'NN'), ('graduat', 'NN'), ('alreadi', 'NN'), ('show', 'VBP'), ('us', 'PRP'), ('capabl', 'JJ'), ('accomplish', 'JJ'), ('goal', 'NN'), ('commit', 'NN'), ('.', '.')]\n",
            "POS Tags for sentence 20: [('hope', 'NN'), ('us', 'PRP'), ('today', 'NN'), ('take', 'VB'), ('person', 'NN'), ('accomplish', 'JJ'), ('exampl', 'NN'), ('anyth', 'NN'), ('truli', 'NN'), ('possibl', 'NN'), ('put', 'VBD'), ('mind', 'NN'), ('.', '.')]\n",
            "POS Tags for sentence 21: [('continu', 'NN'), ('live', 'CD'), (',', ','), ('let', 'VB'), ('us', 'PRP'), ('take', 'VB'), ('new', 'JJ'), ('problem', 'NN'), ('confid', 'NN'), (',', ','), ('know', 'VBP'), ('achiev', 'RB'), ('great', 'JJ'), ('height', 'JJ'), ('equip', 'NN'), ('necessari', 'JJ'), ('tool', 'NN'), ('tackl', 'NN'), ('futur', 'NN'), ('.', '.')]\n",
            "POS Tags for sentence 22: [('road', 'NN'), ('lie', 'NN'), ('ahead', 'RB'), ('‚Äô', 'NNP'), ('easi', 'NN'), ('.', '.')]\n",
            "POS Tags for sentence 23: [('obstacl', 'JJ'), ('miss', 'NN'), ('exit', 'NN'), (',', ','), ('pothol', 'NN'), ('roadblock', 'NN'), ('.', '.')]\n",
            "POS Tags for sentence 24: [('time', 'NN'), ('us', 'PRP'), ('feel', 'VBP'), ('like', 'IN'), ('possibl', 'NN'), ('go', 'VBP'), ('.', '.')]\n",
            "POS Tags for sentence 25: [('time', 'NN'), ('us', 'PRP'), ('think', 'VBP'), ('alon', 'NN'), (',', ','), ('back', 'RB'), ('wall', 'NN'), ('.', '.')]\n",
            "POS Tags for sentence 26: [('alon', 'NN'), ('‚Äî', 'NNP'), ('journey', 'NN'), ('togeth', 'NN'), ('.', '.')]\n",
            "POS Tags for sentence 27: [('‚Äô', 'NNS'), ('make', 'VBP'), ('far', 'RB'), ('‚Äî', 'VB'), ('back', 'RB'), ('?', '.')]\n",
            "POS Tags for sentence 28: [('noth', 'DT'), ('worthwhil', 'JJ'), ('easi', 'NN'), (',', ','), ('includ', 'JJ'), ('make', 'VBP'), ('futur', 'NN'), ('.', '.')]\n",
            "POS Tags for sentence 29: [('‚Äô', 'JJ'), ('mean', 'RBS'), ('give', 'NN'), ('.', '.')]\n",
            "POS Tags for sentence 30: [('keep', 'VB'), ('push', 'NN'), ('know', 'VBP'), ('achiev', 'JJ'), ('dream', 'NN'), (',', ','), ('worth', 'NN'), ('.', '.')]\n",
            "POS Tags for sentence 31: [('day', 'NN'), ('forward', 'RB'), (',', ','), ('let', 'VB'), ('us', 'PRP'), ('make', 'VB'), ('deci', 'JJ'), ('best', 'JJS'), ('interest', 'NN'), ('mind', 'NN'), ('.', '.')]\n",
            "POS Tags for sentence 32: [('let', 'VB'), ('us', 'PRP'), ('believ', 'VB'), ('may', 'MD'), ('reach', 'VB'), ('goal', 'NN'), ('fulfil', 'JJ'), ('dream', 'NN'), ('.', '.')]\n",
            "POS Tags for sentence 33: [('let', 'VB'), ('us', 'PRP'), ('best', 'JJS'), ('may', 'MD'), ('fill', 'VB'), ('live', 'JJ'), ('live', 'NN'), ('closest', 'JJS'), ('us', 'PRP'), ('happi', 'JJ'), ('pride', 'NN'), ('.', '.')]\n",
            "POS Tags for sentence 34: [('‚Äô', 'JJ'), ('alreadi', 'NNS'), ('take', 'VBP'), ('first', 'JJ'), ('step', 'NN'), ('make', 'VBP'), ('ceremoni', 'NN'), ('today', 'NN'), ('‚Äî', 'NNP'), (',', ','), ('‚Äô', 'JJ'), ('time', 'NN'), ('take', 'VB'), ('next', 'JJ'), ('step', 'NN'), ('journey', 'NN'), ('live', 'JJ'), ('begin', 'NN'), ('build', 'VB'), ('futur', 'NN'), ('.', '.')]\n",
            "POS Tags for sentence 35: [('congratul', 'NN'), ('class', 'NN'), ('2013', 'CD'), ('!', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tag_sent=nltk.pos_tag(sentences) #POS in respect to each sentence\n",
        "pos_tag_sent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OslLUSQVhdr",
        "outputId": "cae59c20-4ddd-42b0-c716-6f6e06ecf17d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('good morn , famili , friend , faculti , fellow graduat .', 'NN'),\n",
              " ('well , .', 'NN'),\n",
              " ('accomplish one major ear mileston live : high school graduat .', 'NN'),\n",
              " ('thi major step journey live , one recogn immen signif .', 'NN'),\n",
              " ('act person commit , also one pride .', 'NN'),\n",
              " ('work hard get day , work go wast .', 'NN'),\n",
              " ('high school diploma wonder tool world , one open mani door opportun anyon lucki enough one .',\n",
              "  'NN'),\n",
              " ('graduat end goal ; instead part larger journey life .', 'NN'),\n",
              " ('wherev futur take , let take somewh .', 'NN'),\n",
              " ('life journey , accomplish achiev cour take start point achiev .', 'NN'),\n",
              " ('graduat serv launch point , project us wherev futur mean take us , whether land career , take trade , continu educ colleg vocational/techn school .',\n",
              "  'NN'),\n",
              " ('begin reach star , one person mileston need reach .', 'NN'),\n",
              " ('peopl graduat high school experi one graduat ‚Äî high school .', 'NN'),\n",
              " ('one short ahead us .', 'NN'),\n",
              " ('‚Äô alreadi show commit person growth make ceremoni today , soon , us experi anoth ceremoni graduat program .',\n",
              "  'NNP'),\n",
              " ('say , life journey ‚Äî ‚Äô stop grow get diploma .', 'NN'),\n",
              " ('life grow , program give us new opportun continu grow learn new skill carri us rest live .',\n",
              "  'NN'),\n",
              " ('‚Äô stop !', 'NNP'),\n",
              " ('thi graduat alreadi show us capabl accomplish goal commit .', 'NN'),\n",
              " ('hope us today take person accomplish exampl anyth truli possibl put mind .',\n",
              "  'NN'),\n",
              " ('continu live , let us take new problem confid , know achiev great height equip necessari tool tackl futur .',\n",
              "  'NN'),\n",
              " ('road lie ahead ‚Äô easi .', 'NN'),\n",
              " ('obstacl miss exit , pothol roadblock .', 'NN'),\n",
              " ('time us feel like possibl go .', 'NN'),\n",
              " ('time us think alon , back wall .', 'NN'),\n",
              " ('alon ‚Äî journey togeth .', 'NN'),\n",
              " ('‚Äô make far ‚Äî back ?', 'NNP'),\n",
              " ('noth worthwhil easi , includ make futur .', 'CC'),\n",
              " ('‚Äô mean give .', 'NNP'),\n",
              " ('keep push know achiev dream , worth .', 'NNP'),\n",
              " ('day forward , let us make deci best interest mind .', 'NN'),\n",
              " ('let us believ may reach goal fulfil dream .', 'NN'),\n",
              " ('let us best may fill live live closest us happi pride .', 'NN'),\n",
              " ('‚Äô alreadi take first step make ceremoni today ‚Äî , ‚Äô time take next step journey live begin build futur .',\n",
              "  'NNP'),\n",
              " ('congratul class 2013 !', 'NN')]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#WORD 2 VEC"
      ],
      "metadata": {
        "id": "JnSQD8wAeiQ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gensim"
      ],
      "metadata": {
        "id": "yDkC6f-pV8AN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba1ca5d1-68be-4430-aace-3f06c7152f31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim"
      ],
      "metadata": {
        "id": "fRlpEGeCevJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec, KeyedVectors\n"
      ],
      "metadata": {
        "id": "w8JLGwJfezg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api"
      ],
      "metadata": {
        "id": "zSh9_mArfCW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wv=api.load('word2vec-google-news-300') #Importing google word2vec  model with api\n",
        "vec_king=wv['king']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "HtDFzlgWfTTk",
        "outputId": "859c10f9-9111-4179-dcda-1acc42815cbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[=========-----------------------------------------] 18.5% 308.3/1662.8MB downloaded"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3158677887.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'word2vec-google-news-300'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Importing google word2vec  model with api\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvec_king\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'king'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gensim/downloader.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, return_path)\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m         \u001b[0m_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gensim/downloader.py\u001b[0m in \u001b[0;36m_download\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{fname}.gz\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0mdst_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreporthook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_progress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_calculate_md5_checksum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_get_checksum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    271\u001b[0m                 \u001b[0mblocknum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mreporthook\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m                     \u001b[0mreporthook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocknum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mread\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gensim/downloader.py\u001b[0m in \u001b[0;36m_progress\u001b[0;34m(chunks_downloaded, chunk_size, total_size, part, total_parts)\u001b[0m\n\u001b[1;32m    127\u001b[0m                 round(float(total_size) / (1024 * 1024), 1))\n\u001b[1;32m    128\u001b[0m         )\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         sys.stdout.write(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mflush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0;31m# and give a timeout to avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m                 \u001b[0;31m# write directly to __stderr__ instead of warning because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0;31m# if this is happening sys.stderr may be the problem.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wv['cricket']"
      ],
      "metadata": {
        "id": "apjsjy6Ef6P0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wv.most_similar['cricket']  #It will give similar  words of cricket along with similartity in %"
      ],
      "metadata": {
        "id": "h9zaw5U6gCdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wv.similarity['hockey', 'sports'] #It will show how similar both the words 'hockey' and 'sports' are e.g, 53%"
      ],
      "metadata": {
        "id": "Niiz9lk9gFn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vec=wv['king']-wv['man']+wv['woman']"
      ],
      "metadata": {
        "id": "ZZEf-TIUge_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wv.most_similar([vec])"
      ],
      "metadata": {
        "id": "iWu-eWmxgyCi"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}